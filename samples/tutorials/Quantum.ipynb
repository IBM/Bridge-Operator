{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096d7434",
   "metadata": {},
   "source": [
    "# Quantum\n",
    "\n",
    "This tutorial demonstrates submiting and monitoring a job which is running on external system  - \n",
    "[IBM Quantum Services](https://cloud.ibm.com/docs/quantum-computing). We can submit jobs to a Quantum Service using the Bridge operator or submitting a Kubeflow Pipelines script which uses the Quantum pod, or using the Quantum pod directly. This tutorial will demonstrate the setup and deployment for running a test script, and how to use S3 for file upload and download for all three implementations. See [README](https://github.ibm.com/Accelerated-Discovery/bridge-operator/tree/master/pods/quantum) for details on setting up a IBM Quantum Service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f15ec3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb33658",
   "metadata": {},
   "source": [
    "##  Setup \n",
    "\n",
    "#### S3\n",
    "Create the S3 bucket with input files\n",
    "\n",
    "- Create a test bucket on S3 called \"quantum\" and upload the files sample_vqe.py, metadata.json and parameters.json to /quantum/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580d00d",
   "metadata": {},
   "source": [
    "#### Create environment variables\n",
    "\n",
    "For these tests we need to specify our S3 and resource endpoints and S2 bucket name. If the job script, parameter file and metadata file are in S3 the we also need to provide the ```bucket:folder/filename```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c43676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ENDPOINT=minio-kubeflow.apps.adp-rosa-2.5wcf.p1.openshiftapps.com\n",
      "env: BUCKET=quantum\n",
      "env: RESOURCE_URL=https://us-east.quantum-computing.cloud.ibm.com/\n",
      "env: JOBSCRIPT=adp-bridge-operator-test-bucket-1:data/sample_vqe.py\n",
      "env: SCRIPT_MD=adp-bridge-operator-test-bucket-1:data/metadata.json\n",
      "env: PARAMS=adp-bridge-operator-test-bucket-1:data/parameters.json\n"
     ]
    }
   ],
   "source": [
    "%env ENDPOINT=minio-kubeflow.apps.adp-rosa-2.5wcf.p1.openshiftapps.com\n",
    "%env BUCKET=quantum\n",
    "%env RESOURCE_URL=https://us-east.quantum-computing.cloud.ibm.com/\n",
    "\n",
    "%env JOBSCRIPT=adp-bridge-operator-test-bucket-1:data/sample_vqe.py\n",
    "%env SCRIPT_MD=adp-bridge-operator-test-bucket-1:data/metadata.json\n",
    "%env PARAMS=adp-bridge-operator-test-bucket-1:data/parameters.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d020338",
   "metadata": {},
   "source": [
    "#### Create the S3 and Quantum secrets needed by the pod\n",
    "\n",
    "Edit the S3 and Quantum secret yaml file with credentials to access S3. Then create these secrets in the namespace you wish to run jobs in, e.g. set env variable and  run in bridge-operator-system use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc467931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: S3_SECRET=mysecret-s3\n",
      "env: RESOURCE_SECRET=secret-quantum\n"
     ]
    }
   ],
   "source": [
    "# Define env names for secrets to be used for all jobs\n",
    "%env S3_SECRET=mysecret-s3\n",
    "%env RESOURCE_SECRET=secret-quantum\n",
    "\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/secrets/s3secret.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../core/secrets/quantumsecret.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71b88bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/secret-quantum created\n",
      "secret/mysecret-s3 configured\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/secrets/quantumsecret.yaml -n bridge-operator-system\n",
    "!kubectl apply -f ../core/secrets/s3secret.yaml -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cf8b3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce9037",
   "metadata": {},
   "source": [
    "## 1. Testing the Quantum pod directly\n",
    "\n",
    "Testing of individual pods can be done directly without invoking the Bridge operator.\n",
    "\n",
    "For Quantum the ```samples/tests/quantum/pod.yaml``` specifies\n",
    "- the pod image to use ```quay.io/ibmdpdev/quantum-pod:v0.0.1```\n",
    "- the jobname ```quantumjob```\n",
    "The secret name for both the resource and S3 must be set using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87004a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../test/quantum/pod.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../test/quantum/pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb4871",
   "metadata": {},
   "source": [
    "The configmap yamls are in ```samples/tests/quantum/ ``` and there you must configure \n",
    "- the Minio endpoint\n",
    "- the S3 bucket name\n",
    "- the resource URL\n",
    "\n",
    "Run the following to set the envoirnment variables and create the configmap. Then submit the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdd11176",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../test/quantum/quantum_sample0_cm.yaml \n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../test/quantum/quantum_sample0_cm.yaml \n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../test/quantum/quantum_sample0_cm.yaml\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../test/quantum/quantum_sample0_cm.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d76ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl apply -f ../test/quantum/quantum_sample0_cm.yaml -n bridge-operator-system\n",
    "!kubectl apply -f ../test/quantum/pod.yaml -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3dfb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Monitor the job\n",
    "!kubectl logs quantumjob-pod -n bridge-operator-system\n",
    "!kubectl describe pod quantumjob-pod -n bridge-operator-system\n",
    "# Once the job completes the log file will be in the S3 bucket specified in the configmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806bbb2d",
   "metadata": {},
   "source": [
    "## 2. Bridge operator for Quantum\n",
    "\n",
    "There are three sample yaml files in ```samples/core/operator``` for running jobs to a Quantum Service using the Bridge operator.\n",
    "Before running either job edit the files so that \n",
    "\n",
    "- S3storage: endpoint: corresponds to your S3 endpoint\n",
    "- S3upload: bucket: corresponds to your bucket in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b25514",
   "metadata": {},
   "source": [
    "### Remote script and inline job parameters example \n",
    "The ```job0quantum.yaml``` submits a helloworld job script which is 'remote' and the log output from the job is saved into the S3upload bucket ```<BUCKET_NAME>/quantumjob```. The input variables are defined in the jobparameters dictionary and envoirnment settings and package installations can be specified in ```scriptmetadata```.\n",
    "To edit the yaml and run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a7d11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../core/operator/job0quantum.yaml\n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../core/operator/job0quantum.yaml\n",
    "\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/operator/job0quantum.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../core/operator/job0quantum.yaml\n",
    "\n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job0quantum.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ebd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridgejob.bridgejob.ibm.com/bridgejob-quantum created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/operator/job0quantum.yaml -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da09704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the pod logs\n",
    "!kubectl describe pod bridgejob-quantum-bridge-pod -n bridge-operator-system\n",
    "!kubectl logs bridgejob-quantum-bridge-pod -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c992ec",
   "metadata": {},
   "source": [
    "### Inline script and job parameters example\n",
    "The ```job1quantum.yaml``` submits a job script which is inline. The log output from the job is saved into the S3upload bucket ```<BUCKET_NAME>/quantumjob```. \n",
    "To run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "186de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../core/operator/job1quantum.yaml\n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../core/operator/job1quantum.yaml\n",
    "\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/operator/job1quantum.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../core/operator/job1quantum.yaml\n",
    "\n",
    "\n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job1quantum.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d66bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f ../core/operator/job1quantum.yaml -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed47aad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the pod logs\n",
    "!kubectl describe pod bridgejob-quantum-bridge-pod -n bridge-operator-system \n",
    "!kubectl logs bridgejob-quantum-bridge-pod -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdba642",
   "metadata": {},
   "source": [
    "### Script and job parameters in S3 example\n",
    "The ```job2quantum.yaml``` submits a job script which is in S3 at ```<BUCKET_NAME>/data/sample_vqe.py```. The log output from the job is saved into the S3upload bucket ```<BUCKET_NAME>/quantumjob```. The input variables to the python script are defined in the ```parameters.json``` file in S3 at ```<BUCKET_NAME>/data/```  and envoirnment settings and package installations are specified in ```metadata.json``` in S3 at ```<BUCKET_NAME>/data/```.\n",
    "To run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2ffc036",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../core/operator/job2quantum.yaml\n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../core/operator/job2quantum.yaml\n",
    "!sed -i '' \"s#{{JOBSCRIPT}}#$JOBSCRIPT#g\" ../core/operator/job2quantum.yaml \n",
    "!sed -i '' \"s#{{SCRIPT}}#$SCRIPT_MD#g\" ../core/operator/job2quantum.yaml \n",
    "!sed -i '' \"s#{{PARAMS}}#$PARAMS#g\" ../core/operator/job2quantum.yaml \n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/operator/job2quantum.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../core/operator/job2quantum.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job2quantum.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b38fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl apply -f ../core/operator/job2quantum.yaml -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c7a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the pod logs\n",
    "!kubectl describe pod bridgejob-quantum-bridge-pod -n bridge-operator-system \n",
    "!kubectl logs bridgejob-quantum-bridge-pod -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a1287",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab9483",
   "metadata": {},
   "source": [
    "## 3. KubeFlow Pipelines\n",
    "\n",
    "These examples assume you have access to a KFP with Tekton installation where you can submit and run jobs or upload pipelines to the KFP UI. See e.g. ``` bridge-operator/kubeflow/```\n",
    "\n",
    "The credentials for S3 and the external resource should be saved to the kubeflow namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bc7c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/secret-quantum configured\n",
      "secret/mysecret-s3 configured\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/secrets/quantumsecret.yaml -n kubeflow\n",
    "!kubectl apply -f ../core/secrets/s3secret.yaml -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb120ce",
   "metadata": {},
   "source": [
    "The implementation with KubeFlow Pipelines uses a general ```bridge-pipeline``` given in ```kubeflow/bridge_pipeline_handler.py``` and the specific implementation for Quantum is in ```kubeflow/implementations/quantum_invoker.py```\n",
    "\n",
    "1. compile the bridge pipeline\n",
    "\n",
    "``` $ python bridge_pipeline_handler.py ```\n",
    "\n",
    "2. Upload the generated yaml to the KFP UI > pipelines\n",
    "\n",
    "\n",
    "3. Run ```kubeflow/implementations/quantum_invoker.py``` providing\n",
    "\n",
    "- a host endpoint for KFP\n",
    "- a ```s3endpoint``` for S3 \n",
    "- a ```s3uploadbucket``` name \n",
    "- a bucket name in ```jobparams```, ```script``` and ```scriptmd``` if ```scriptlocation``` and ```scriptextraloc``` are 'S3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the job\n",
    "!python ../../kubeflow/implementations/quantum_invoker.py --kfphost=<KFP_HOST> \\\n",
    "                                                      --s3endpoint=<s3ENDPOINT> --s3uploadbucket=<BUCKET> \\\n",
    "                                                      --script=<BUCKET:SCRIPT> --scriptmd=<BUCKET:SCRIPTMD> \\\n",
    "                                                      --jobparams=<BUCKET:JOBPARAMS>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9f011",
   "metadata": {},
   "source": [
    "Output from the KFP job can be viewed in the UI and the logs are uploaded to S3 ```<BUCKET>/quantumjob```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdec5d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f841f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
