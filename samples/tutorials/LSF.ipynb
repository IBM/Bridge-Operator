{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096d7434",
   "metadata": {},
   "source": [
    "# HPC Cluster - LSF\n",
    "\n",
    "This tutorial demonstrates submiting and monitoring a job which is running an external LSF system. We can submit jobs to a LSF HPC cluster using the Bridge operator or submitting a Kubeflow Pipelines script which uses the LSF pod, or using the LSF pod directly. This tutorial will demonstrate the setup and deployment for running a test script, and how to use S3 for file upload and download for all three implementations. See also [README](https://github.ibm.com/Accelerated-Discovery/bridge-operator/tree/master/pods/lsf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f15ec3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb33658",
   "metadata": {},
   "source": [
    "##  Setup \n",
    "\n",
    "#### S3\n",
    "Create the S3 bucket with input files\n",
    "\n",
    "- Create a test bucket on S3 called \"mybucket\" and upload the sample batch script `lsf_batch.sh` and a test file `test.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580d00d",
   "metadata": {},
   "source": [
    "#### Create environment variables\n",
    "\n",
    "For these tests we need to specify our S3 and resource endpoints and S3 bucket name. If the job script, parameter file and metadata file are in S3 the we also need to provide the ```bucket:folder/filename```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c43676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ENDPOINT=minio-kubeflow.apps.adp-rosa-2.5wcf.p1.openshiftapps.com\n",
      "env: BUCKET=mybucket\n",
      "env: RESOURCE_URL=https://161.156.200.86:8443/platform/\n",
      "env: JOBSCRIPT=mybucket:lsf_batch.sh\n",
      "env: ADDITIONALDATA=mybucket:test.txt\n",
      "env: REMOTEJOBSCRIPT=/home/lsfadmin/shared/tests/batch.sh\n",
      "env: UPLOADFILE=sample.out\n",
      "env: S3_SECRET=mysecret-s3\n",
      "env: RESOURCE_SECRET=mysecret\n"
     ]
    }
   ],
   "source": [
    "%env ENDPOINT=minio-kubeflow.apps.adp-rosa-2.5wcf.p1.openshiftapps.com\n",
    "%env BUCKET=mybucket\n",
    "%env RESOURCE_URL=https://161.156.200.86:8443/platform/\n",
    "\n",
    "%env JOBSCRIPT=mybucket:lsf_batch.sh\n",
    "%env ADDITIONALDATA=mybucket:test.txt\n",
    "    \n",
    "%env REMOTEJOBSCRIPT=/home/lsfadmin/shared/tests/batch.sh\n",
    "%env UPLOADFILE=sample.out\n",
    "\n",
    "%env S3_SECRET=mysecret-s3\n",
    "%env RESOURCE_SECRET=mysecret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d020338",
   "metadata": {},
   "source": [
    "#### Create the S3 and LSF secrets needed by the pod\n",
    "\n",
    "Edit the S3 and LSF secret yaml file with credentials to access S3. Then create these secrets in the namespace you wish to run jobs in, e.g. set env variable and  run in bridge-operator-system use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc467931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: S3_SECRET=mysecret-s3\n",
      "env: RESOURCE_SECRET=mysecret\n"
     ]
    }
   ],
   "source": [
    "# Define env names for secrets to be used for all jobs\n",
    "\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/secrets/s3secret.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../core/secrets/lsfsecret.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b88bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f ../core/secrets/lsfsecret.yaml -n bridge-operator-system\n",
    "!kubectl apply -f ../core/secrets/s3secret.yaml -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cf8b3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce9037",
   "metadata": {},
   "source": [
    "## 1. Testing the LSF pod directly\n",
    "\n",
    "Testing of individual pods can be done directly without invoking the Bridge operator.\n",
    "\n",
    "For LSF the ```samples/tests/lsf/pod.yaml``` specifies\n",
    "- the pod image to use ```quay.io/ibmdpdev/lsf-pod:v0.0.1```\n",
    "- the jobname ```hpcjob```\n",
    "The secret name for both the resource and S3 must be set using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87004a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../test/lsf/pod.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../test/lsf/pod.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb4871",
   "metadata": {},
   "source": [
    "The configmap yamls are in ```samples/tests/lsf/ ``` and there you must configure \n",
    "- the Minio endpoint\n",
    "- the S3 bucket name\n",
    "- the resource URL\n",
    "\n",
    "Run the following to set the envoirnment variables and create the configmap. Then submit the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd11176",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../test/lsf/hpcjob-sample0_cm.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../test/lsf/hpcjob-sample0_cm.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../test/lsf/hpcjob-sample0_cm.yaml\n",
    "!sed -i '' \"s#{{REMOTEJOBSCRIPT}}#$REMOTEJOBSCRIPT#g\" ../test/lsf/hpcjob-sample0_cm.yaml\n",
    "!sed -i '' \"s#{{ADDITIONALDATA}}#$ADDITIONALDATA#g\" ../test/lsf/hpcjob-sample0_cm.yaml\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../test/lsf/hpcjob-sample0_cm.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d76ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/hpcjob-bridge-cm created\n",
      "serviceaccount/hpc-cm-viewer unchanged\n",
      "role.rbac.authorization.k8s.io/hpc-cm-viewer-role unchanged\n",
      "rolebinding.rbac.authorization.k8s.io/hpc-cm-viewer-rolebinding unchanged\n",
      "pod/hpcjob-pod created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../test/lsf/hpcjob-sample0_cm.yaml\n",
    "!kubectl apply -f ../test/lsf/pod.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monitor the job\n",
    "!kubectl logs hpcjob-pod\n",
    "!kubectl describe pod hpcjob-pod\n",
    "# Once the job completes the log file will be in the S3 bucket specified in the configmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d6ab4",
   "metadata": {},
   "source": [
    "Testing inline script and upload results to S3. Note that the results are uploaded to the S3 location ```mybucket:hpcjob/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dcc2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../test/lsf/hpcjob-sample1_cm.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../test/lsf/hpcjob-sample1_cm.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../test/lsf/hpcjob-sample1_cm.yaml\n",
    "!sed -i '' \"s#{{REMOTEJOBSCRIPT}}#$REMOTEJOBSCRIPT#g\" ../test/lsf/hpcjob-sample1_cm.yaml\n",
    "!sed -i '' \"s#{{ADDITIONALDATA}}#$ADDITIONALDATA#g\" ../test/lsf/hpcjob-sample1_cm.yaml\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../test/lsf/hpcjob-sample1_cm.yaml\n",
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../test/lsf/hpcjob-sample1_cm.yaml\n",
    "!sed -i '' \"s#{{UPLOADFILE}}#$UPLOADFILE#g\" ../test/lsf/hpcjob-sample1_cm.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b9c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/hpcjob-bridge-cm created\n",
      "serviceaccount/hpc-cm-viewer unchanged\n",
      "role.rbac.authorization.k8s.io/hpc-cm-viewer-role unchanged\n",
      "rolebinding.rbac.authorization.k8s.io/hpc-cm-viewer-rolebinding unchanged\n",
      "pod/hpcjob-pod created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../test/lsf/hpcjob-sample1_cm.yaml\n",
    "!kubectl apply -f ../test/lsf/pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806bbb2d",
   "metadata": {},
   "source": [
    "## 2. Bridge operator for LSF\n",
    "\n",
    "There are three sample yaml files in ```samples/core/operator``` for running jobs to a LSF cluster using the Bridge operator.\n",
    "Before running either job edit the files so that \n",
    "\n",
    "- S3storage: endpoint: corresponds to your S3 endpoint\n",
    "- S3upload: bucket: corresponds to your bucket in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b25514",
   "metadata": {},
   "source": [
    "### Run remote script on LSF cluster, download input file from S3 and upload output file to S3 example \n",
    "The ```job0lsf.yaml``` submits a simple job script which is 'remote' on the LSF cluster.\n",
    "To edit the yaml and run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a7d11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job0lsf.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../core/operator/job0lsf.yaml\n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../core/operator/job0lsf.yaml\n",
    "!sed -i '' \"s#{{REMOTEJOBSCRIPT}}#$REMOTEJOBSCRIPT#g\" ../core/operator/job0lsf.yaml\n",
    "!sed -i '' \"s#{{ADDITIONALDATA}}#$ADDITIONALDATA#g\" ../core/operator/job0lsf.yaml\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/operator/job0lsf.yaml\n",
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../core/operator/job0lsf.yaml\n",
    "!sed -i '' \"s#{{UPLOADFILE}}#$UPLOADFILE#g\" ../core/operator/job0lsf.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89ebd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridgejob.bridgejob.ibm.com/lsfjob created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/operator/job0lsf.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da09704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0c1e915",
   "metadata": {},
   "source": [
    "### S3 script example\n",
    "The ```job2lsf.yaml``` submits a job script which is in an S3 bucket. \n",
    "To run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05bcfe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job2lsf.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../core/operator/job2lsf.yaml\n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../core/operator/job2lsf.yaml\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/operator/job2lsf.yaml\n",
    "!sed -i '' \"s#{{JOBSCRIPT}}#$JOBSCRIPT#g\" ../core/operator/job2lsf.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ceafa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridgejob.bridgejob.ibm.com/lsfjob created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/operator/job2lsf.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a1287",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab9483",
   "metadata": {},
   "source": [
    "## 3. KubeFlow Pipelines\n",
    "\n",
    "These examples assume you have access to a KFP with Tekton installation where you can submit and run jobs or upload pipelines to the KFP UI. See e.g. ``` bridge-operator/kubeflow/```\n",
    "\n",
    "The credentials for S3 and the external resource should be saved to the kubeflow namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bc7c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/mysecret configured\n",
      "secret/mysecret-s3 configured\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/secrets/lsfsecret.yaml-COPY -n kubeflow\n",
    "!kubectl apply -f ../core/secrets/s3secret.yaml-COPY -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb120ce",
   "metadata": {},
   "source": [
    "The implementation with KubeFlow Pipelines uses a general ```bridge-pipeline``` given in ```kubeflow/bridge_pipeline_handler.py``` and the specific implementation for LSF is in ```kubeflow/implementations/lsf_invoker.py```\n",
    "\n",
    "1. compile the bridge pipeline\n",
    "\n",
    "``` $ python bridge_pipeline_handler.py ```\n",
    "\n",
    "2. Upload the generated yaml to the KFP UI > pipelines\n",
    "\n",
    "\n",
    "3. Run ```kubeflow/implementations/lsf_invoker.py``` providing\n",
    "\n",
    "- a host endpoint for KFP\n",
    "- a ```s3endpoint``` for S3 \n",
    "- a ```s3_secret``` name \n",
    "- a ```resource_secret``` name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the job\n",
    "!python ../../kubeflow/implementations/lsf_invoker.py --kfphost=<KFP_HOST> \\\n",
    "                                                      --s3endpoint=<s3ENDPOINT> --s3_secret=<S3_SECRET> \\\n",
    "                                                      --script=<BUCKET:SCRIPT> --resource_secret=<RESOURCE_SECRET>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdec5d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f841f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
