{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096d7434",
   "metadata": {},
   "source": [
    "# Ray\n",
    "\n",
    "This tutorial demonstrates submiting and monitoring a job which is running on an external \n",
    "[Ray](https://www.ray.io/) cluster. We can submit jobs to an external Ray cluster using the Bridge operator or submitting a Kubeflow Pipelines script which uses the Ray pod, or using the Ray pod directly. This tutorial will demonstrate the setup and deployment for running a test script, and how to use S3 for file upload and download for all three implementations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f15ec3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb33658",
   "metadata": {},
   "source": [
    "##  Setup \n",
    "\n",
    "#### S3\n",
    "Create the S3 bucket with input files\n",
    "\n",
    "- Create a test bucket on S3 called \"mybucket\" and upload the files parameters.json, metadata.json and code.py to /mybucket/ray/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c2fe0",
   "metadata": {},
   "source": [
    "#### Create environment variables\n",
    "\n",
    "For these tests we need to specify our S3 and resource endpoints and S2 bucket name. If the job script, parameter file and metadata file are in S3 the we also need to provide the ```bucket:folder/filename```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e77da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env RESOURCE_URL=http://10.0.57.51:8265\n",
    "%env ENDPOINT=minio-kubeflow.apps.adp-rosa-2.5wcf.p1.openshiftapps.com\n",
    "%env BUCKET=mybucket\n",
    "%env JOBSCRIPT=mybucket:ray/code.py\n",
    "%env SCRIPT_MD=mybucket:ray/metadata.json\n",
    "%env PARAMS=mybucket:ray/parameters.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353bd202",
   "metadata": {},
   "source": [
    "#### Create the S3 and Ray secrets needed by the pod\n",
    "\n",
    "Edit the S3 secret yaml file with credentials to access S3. Then create these secrets in the namespace you wish to run jobs in, e.g. to run in bridge-operator-system use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define env names for secrets to be used for all jobs\n",
    "%env S3_SECRET=mysecret-s3\n",
    "%env RESOURCE_SECRET=mysecret\n",
    "\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/secrets/s3secret.yaml \n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../core/secrets/raysecret.yaml \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71b88bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/mysecret created\n",
      "secret/mysecret-s3 created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/secrets/raysecret.yaml -n bridge-operator-system\n",
    "!kubectl apply -f ../core/secrets/s3secret.yaml -n bridge-operator-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cf8b3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce9037",
   "metadata": {},
   "source": [
    "## 1. Testing the Ray pod directly\n",
    "\n",
    "Testing of individual pods can be done directly without invoking the Bridge operator.\n",
    "\n",
    "For Ray the ```samples/tests/ray/ray_job.yaml``` specifies\n",
    "- the pod image to use ```quay.io/ibmdpdev/ray-pod:v0.0.1```\n",
    "- the configmap ```rayjob-bridge-cm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f237de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../test/ray/ray_job.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_SECRET}}#$RESOURCE_SECRET#g\" ../test/ray/ray_job.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6042f5",
   "metadata": {},
   "source": [
    "The configmap yamls are in ```samples/tests/ray/ ``` and there you must specify\n",
    "- with the address of the Ray cluster\n",
    "- the Minio endpoint\n",
    "- the bucket name\n",
    "\n",
    "Edit the yamls and create the configmap. Then submit the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd11176",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../test/ray/ray_sample0_cm.yaml \n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../test/ray/ray_sample0_cm.yaml \n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../test/ray/ray_sample0_cm.yaml\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../test/ray/ray_sample0_cm.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d76ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f ../test/ray/ray_sample0_cm.yaml \n",
    "!kubectl apply -f ../test/ray/ray_job.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monitor the job\n",
    "!kubectl logs hpcjob-pod\n",
    "!kubectl describe pod hpcjob-pod\n",
    "# Once the job completes the log file will be in the S3 bucket specified in the configmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806bbb2d",
   "metadata": {},
   "source": [
    "## 2. Bridge operator for Ray\n",
    "\n",
    "There are two sample yaml files in ```samples/core/operator``` for running Ray jobs using the Bridge operator.\n",
    "Before running either job edit the files so that \n",
    "\n",
    "- resourceURL corresponds to your Ray cluster\n",
    "- S3storage: endpoint: corresponds to your S3 endpoint\n",
    "- S3upload: bucket: corresponds to your bucket in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b25514",
   "metadata": {},
   "source": [
    "### Inline script and job parameters example \n",
    "The ```job0ray.yaml``` submits a python job script which is given 'inline' and the log output from the job is saved into the S3upload bucket ```<BUCKET_NAME>/bridgejob-ray```. The input variables to the python script are defined in the jobparameters dictionary and envoirnment settings and package installations can be specified in ```scriptmetadata```.\n",
    "To edit the yaml and run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../core/operator/job0ray.yaml \n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../core/operator/job0ray.yaml \n",
    "!sed  -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job0ray.yaml\n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job0ray.yaml\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/operator/job0ray.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89ebd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridgejob.bridgeoperator.ibm.com/bridgejob-ray created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/operator/job0ray.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da09704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the pod logs\n",
    "!kubectl describe pod bridgejob-ray-bridge-pod\n",
    "!kubectl logs bridgejob-ray-bridge-pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c992ec",
   "metadata": {},
   "source": [
    "### Script and job parameters in S3 example\n",
    "The ```job1ray.yaml``` submits a python job script which is in S3 at ```<BUCKET_NAME>/ray/code.py```. The log output from the job is saved into the S3upload bucket ```<BUCKET_NAME>/bridgejob-ray```. The input variables to the python script are defined in the ```parameters.json``` file in S3 at ```<BUCKET_NAME>/ray/```  and envoirnment settings and package installations are specified in ```metadata.json``` in S3 at ```<BUCKET_NAME>/ray/```.\n",
    "To run the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186de7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '' \"s#{{BUCKET}}#$BUCKET#g\" ../core/operator/job1ray.yaml \n",
    "!sed -i '' \"s#{{ENDPOINT}}#$ENDPOINT#g\" ../core/operator/job1ray.yaml \n",
    "!sed  -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job1ray.yaml \n",
    "!sed -i '' \"s#{{JOBSCRIPT}}#$JOBSCRIPT#g\" ../core/operator/job1ray.yaml \n",
    "!sed -i '' \"s#{{SCRIPT_MD}}#$SCRIPT_MD#g\" ../core/operator/job1ray.yaml \n",
    "!sed -i '' \"s#{{PARAMS}}#$PARAMS#g\" ../core/operator/job1ray.yaml \n",
    "!sed -i '' \"s#{{RESOURCE_URL}}#$RESOURCE_URL#g\" ../core/operator/job1ray.yaml\n",
    "!sed -i '' \"s#{{S3_SECRET}}#$S3_SECRET#g\" ../core/operator/job1ray.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1d66bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridgejob.bridgeoperator.ibm.com/bridgejob-ray unchanged\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/operator/job1ray.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed47aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the pod logs\n",
    "!kubectl describe pod bridgejob-ray-bridge-pod\n",
    "!kubectl logs bridgejob-ray-bridge-pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a1287",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab9483",
   "metadata": {},
   "source": [
    "## 3. KubeFlow Pipelines\n",
    "\n",
    "These examples assume you have access to a KFP with Tekton installation where you can submit and run jobs or upload pipelines to the KFP UI. See e.g. ``` bridge-operator/kubeflow/```\n",
    "\n",
    "The credentials for S3 and the external resource should be saved to the kubeflow namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc7c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/mysecret configured\n",
      "secret/mysecret-s3 configured\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../core/secrets/raysecret.yaml -n kubeflow\n",
    "!kubectl apply -f ../core/secrets/s3secret.yaml -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb120ce",
   "metadata": {},
   "source": [
    "The implementation with KubeFlow Pipelines uses a general ```bridge-pipeline``` given in ```kubeflow/bridge_pipeline_handler.py``` and the specific implementation for Ray is in ```kubeflow/implementations/ray_invoker.py```\n",
    "\n",
    "1. compile the bridge pipeline\n",
    "\n",
    "``` $ python bridge_pipeline_handler.py ```\n",
    "\n",
    "2. Upload the generated yaml to the KFP UI > pipelines\n",
    "\n",
    "\n",
    "3. Run ```kubeflow/implementations/ray_invoker.py``` providing\n",
    "\n",
    "- a host endpoint for KFP\n",
    "- a ```RESOURCEURL``` for the Ray cluster \n",
    "- a ```s3endpoint``` for S3 \n",
    "- a ```s3uploadbucket``` name \n",
    "- a bucket name in ```jobparams```, ```script``` and ```scriptmd``` if ```scriptlocation``` and ```scriptextraloc``` are 'S3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the job\n",
    "!python ../../kubeflow/implementations/ray_invoker.py --kfphost=<KFP_HOST> --resource_url=<RESOURCE_URL> \\\n",
    "                                                      --s3endpoint=<s3ENDPOINT> --s3uploadbucket=<BUCKET> \\\n",
    "                                                      --script=<BUCKET:SCRIPT> --scriptmd=<BUCKET:SCRIPTMD> \\\n",
    "                                                      --jobparams=<BUCKET:JOBPARAMS>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9f011",
   "metadata": {},
   "source": [
    "Output from the KFP job can be viewed in the UI and the logs are uploaded to S3 ```<BUCKET>/rayjob-kfp```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdec5d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f841f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
